# Transport Layer

Residing between the application and network layers, the transport layer is a central piece of the layered network architecture. It has the critical role of providing communication services directly to the application processes running on different hosts.

---

## Introduction and Transport-Layer Services

A transport-layer protocol provides for **logical communication** between application processes running on different hosts. By *logical communication*, we  mean that from an application’s perspective, it is as if the hosts running the processes were directly connected; in reality, the hosts may be on opposite sides of the planet, connected via numerous routers and a wide range of link types. Application processes use the logical communication provided by the transport layer to send messages to each other, free from the worry of the details of the physical infrastructure used to carry these messages.

#### Relationship Between Transport and Network Layers

![3.1.png](A:\PLAYGROUND\gettingstarted-computernetworking\a_top_down_approach\Images\3.1.png)

Recall that the transport layer lies just above the network layer in the protocol stack. Whereas a transport-layer protocol provides logical communication between processes running on different hosts, a network-layer protocol provides logical communication between hosts.

there are two households, one on the East Coast and the other on the West Coast, each with a dozen kids who write letters to their cousins in the other household. The kids send 144 letters in total every week, delivered by the traditional postal service in separate envelopes.

Within each household, there is one kid, Ann in the West Coast house and Bill in the East Coast house, who is responsible for collecting and distributing the mail. Ann and Bill act as the transport-layer protocol, handling the communication among the cousins (processes) in their respective households. They pick up mail from their brothers and sisters and deliver mail to them within their own household.

The postal service, which moves mail from house to house, serves as the network-layer protocol, providing logical communication between the two households (hosts or end systems). Ann and Bill are part of the end-to-end delivery process, representing the transport layer, from the cousins' perspective.

> application messages = letters in envelopes
> processes = cousins
> hosts (also called end systems) = houses
> transport-layer protocol = Ann and Bill
> network-layer protocol = postal service (including mail carriers)

The analogy of cousins writing letters to each other in two households, with Ann and Bill responsible for mail collection and distribution, is used to explain how transport-layer protocols relate to the network layer. Just as Ann and Bill handle the internal mail collection and delivery within their respective households, transport-layer protocols operate within the end systems (hosts) and handle the movement of messages between application processes and the network edge (network layer).

It's emphasized that transport-layer protocols do not have control over how messages are moved within the network core, just as Ann and Bill do not have control over sorting mail in intermediate mail centers. Intermediate routers in the network do not recognize or act on information added by the transport layer.

The analogy also highlights that different transport protocols may offer different service models to applications, similar to how Ann and Bill may provide different services compared to their substitute cousins Susan and Harvey. The possible services that Ann and Bill can provide are constrained by the services offered by the postal service, just as the services offered by a transport protocol may be constrained by the service model of the underlying network-layer protocol.

#### Overview of the Transport Layer in the internet

One of these protocols is **UDP (User Datagram Protocol)**, which provides an unreliable, connectionless service to the invoking application. The second of these protocols is **TCP (Transmission Control Protocol)**, which provides a reliable, connection-oriented service to the invoking application.

To simplify terminology, we refer to the transport-layer packet as a segment. We mention, however, that the Internet literature (for example, the RFCs) also refers to the transport-layer packet for TCP as a segment but often refers to the packet for UDP as a datagram. we believe that it is less confusing to refer to both TCP and UDP packets as segments,  and reserve the term datagram for the network-layer packet.

The Internet’s network-layer protocol has a  name IP, for Internet Protocol. IP provides logical communication between hosts. The IP service model is a **best-effort delivery service**. This means that IP makes its “best effort” to deliver segments between communicating hosts, but it `makes no guarantees`. For these reasons, IP is said to be an **unreliable service**.

TCP. The most fundamental responsibility of UDP and TCP is to extend IP’s delivery service between two end systems to a delivery service between two processes running on the end systems. Extending host-to-host delivery to process-to-process delivery is called **transport-layer multiplexing** and **demultiplexing**. In particular, like IP, UDP is an unreliable service it does not guarantee that data sent by one process will arrive intact (or at all!) to the destination process.

TCP, on the other hand, offers several additional services to applications. First and foremost, it provides **reliable data transfer**. Using flow control, sequence numbers, acknowledgments, and timers, , TCP ensures that data is delivered from sending process to receiving process, correctly and in order. TCP thus converts IP’s unreliable service between end systems into a reliable data transport service between processes. TCP also provides **congestion control**.

TCP congestion control prevents any one TCP connection from swamping the links and routers between communicating hosts with an excessive amount of traffic. TCP strives to give each connection traversing a congested link an equal share of the link bandwidth. This is done by regulating the rate at which the sending sides of TCP connections can send traffic into the network. UDP traffic, on the other hand, is unregulated. An application using UDP transport can send at any rate it pleases, for as long as it pleases.

---

## Multiplexing and Demultiplexing

At the destination host, the transport layer receives segments from the network layer just below. The transport layer has the responsibility of delivering the data in these segments to the appropriate application process running in the host. Let’s take a look at an example. Suppose you are sitting in front of your computer, and you are downloading Web pages while running one FTP session and two Telnet sessions. You therefore have four network application processes running—two Telnet processes, one FTP process, and one HTTP process. When the transport layer in your computer receives data from the network layer below, it needs to direct the received data to one of these four processes.

a process (as part of a network application) can have one or more **sockets**, doors through which data passes from the network to the process and through which data passes from the process to the network. the transport layer in the receiving host does not actually deliver data directly to a process, but instead to an intermediary socket. Because at any given time there can be more than one socket in the receiving host, each socket has a unique identifier. The format of the identifier depends on whether the socket is a UDP or a TCP socket, as we’ll discuss shortly.

Now let’s consider how a receiving host directs an incoming transport-layer segment to the appropriate socket. Each transport-layer segment has a set of fields in the segment for this purpose. At the receiving end, the transport layer examines these fields to identify the receiving socket and then directs the segment to that socket.

> This job of delivering the data in a transport-layer segment to the correct socket is called **demultiplexing**. 
> 
> The job of gathering data chunks at the source host from different sockets, encapsulating each data chunk with header information (that will later be used in demultiplexing) to create segments, and passing the segments to the network layer is called **multiplexing**.

![3.2.png](A:\PLAYGROUND\gettingstarted-computernetworking\a_top_down_approach\Images\3.2.png)

Must demultiplex segments arriving from the network layer below to either process P1 or P2 above; 2 above; this is done by directing the arriving segment’s data to the corresponding process’s socket. The transport layer in the middle host must also gather outgoing data from these sockets, form transport-layer segments, and pass these segments down to the network layer. Although we have introduced multiplexing and demultiplexing in the context of the Internet transport protocols, it’s important to realize that they are concerns whenever a single protocol at one layer (at the transport layer or elsewhere) is used by multiple protocols at the next higher layer.

> To illustrate the demultiplexing job, recall the household analogy in the previous section. Each of the kids is identified by his or her name. When Bill receives a batch of mail from the mail carrier, he performs a demultiplexing operation by observing to whom the letters are addressed and then hand delivering the mail to his brothers and sisters. Ann performs a multiplexing operation when she collects letters from her brothers and sisters and gives the collected mail to the mail person.

We know that transport-layer multiplexing requires:

1. That sockets have unique identifiers.

2. That each segment have special fields that indicate the socket to which the segment is to be delivered.

![3.3.png](A:\PLAYGROUND\gettingstarted-computernetworking\a_top_down_approach\Images\3.3.png)

These special fields, illustrated are the **source port number field** and the **destination port number field**. Each port number is a 16-bit number, ranging from 0 to 65535. The port numbers ranging from 0 to 1023 are called **well-known port numbers** and are restricted, which means that they are reserved for use by well-known application protocols such as HTTP (which uses port number 80) and FTP (which uses port number 21).

It should now be clear how the transport layer could implement the demultiplexing service: Each socket in the host could be assigned a port number, and when a segment arrives at the host, the transport layer examines the destination port number in the segment and directs the segment to the corresponding socket. The segment’s data then passes through the socket into the attached process.

#### Connectionless Multiplexing and Demultiplexing

Python program running in a host can create a UDP socket with the line

```python
clientSocket = socket(AF_INET, SOCK_DGRAM)
```

When a UDP socket is created in this manner, the transport layer automatically assigns a port number to the socket. In particular, the transport layer assigns a port number in the range 1024 to 65535 that is currently not being used by any other UDP port in the host. Alternatively, we can add a line into our Python program after we create the socket to associate a specific port number (say, 19157) to this UDP socket via the socket bind() method:

```python
clientSocket.bind((’’, 19157))
```

It is important to note that a UDP socket is fully identified by a two-tuple consisting of a destination IP address and a destination port number. As a consequence, if two UDP segments have different source IP addresses and/or source port numbers, but have the same destination IP address and destination port number, then the two segments will be directed to the same destination process via the same destination socket.

#### Connection-Oriented Multiplexing and Demultiplexing

One subtle difference between a TCP socket and a UDP socket is that a TCP socket is identified by a four-tuple: (source IP address, source port number, destination IP address, destination port number). Thus, when a TCP segment arrives from the network to a host, the host uses all four values to direct (demultiplex) the segment to the appropriate socket.

![3.4.png](A:\PLAYGROUND\gettingstarted-computernetworking\a_top_down_approach\Images\3.4.png)

To gain further insight, let’s reconsider the TCP client-server programming:

- The TCP server application has a “welcoming socket,” that waits for connectionestablishment requests from TCP clients on port number 12000.

- The TCP client creates a socket and sends a connection establishment request 
  segment with the lines:
  
  ```python
  clientSocket = socket(AF_INET, SOCK_STREAM)
  clientSocket.connect((serverName,12000))
  ```

- A connection-establishment request is nothing more than a TCP segment with destination port number 12000 and a special connection-establishment bit set in the TCP header. The segment also includes a source port number that was chosen by the client.

- When the host operating system of the computer running the server process receives the incoming connection-request segment with destination port 12000, it locates the server process that is waiting to accept a connection on port number 12000. The server process then creates a new socket:
  
  ```python
  connectionSocket, addr = serverSocket.accept()
  ```

- Also, the transport layer at the server notes the following four values in the connection-request segment: (1) the source port number in the segment, (2) the IP address of the source host, (3) the destination port number in the segment, and (4) its own IP address. The newly created connection socket is identified by these four values; all subsequently arriving segments whose source port, source IP address, destination port, and destination IP address match these four values will be demultiplexed to this socket. With the TCP connection now in place, the client and server can now send data to each other.

The server host may support many simultaneous TCP connection sockets, with each socket attached to a process, and with each socket identified by its own four tuple. When a TCP segment arrives at the host, all four fields (source IP address, source port, destination IP address, destination port) are used to direct (demultiplex) the segment to the appropriate socket.

![3.5.png](A:\PLAYGROUND\gettingstarted-computernetworking\a_top_down_approach\Images\3.5.png)

Host C initiates two HTTP sessions to server B, and Host A initiates one HTTP session to B. Hosts A and C and server B each have their own unique IP address—A, C, and B, respectively. Host C assigns two different source port numbers (26145 and 7532) to its two HTTP connections. Because Host A is choosing source port numbers independently of C, it might also assign a source port of 26145 to its HTTP connection. But this is not a problem—server B will still be able to correctly demultiplex the two connections having the same source port number, since the two connections have different source IP addresses.

#### Web Servers and TCP

Consider a host running a Web server, such as an Apache Web server, on port 80. When clients (for example, browsers) send segments to the server, all segments will have destination port 80. In particular, both the initial connection-establishment segments and the segments carrying HTTP request messages will have destination port 80. As we have just described, the server distinguishes the segments from the different clients using source IP addresses and source port numbers.

Each of these processes has its own connection socket through which HTTP requests arrive and HTTP responses are sent. We mention, however, that there is not always a one-to-one correspondence between connection sockets and processes. In fact, today’s high-performing Web servers often use only one process, and create a new thread with a new connection socket for each new client connection.

If the client and server are using persistent HTTP, then throughout the duration of the persistent connection the client and server exchange HTTP messages via the same server socket. However, if the client and server use non-persistent HTTP, then a new TCP connection is created and closed for every request/response, and hence a new socket is created and later closed for every request/response. This frequent creating and closing of sockets can severely impact the performance of a busy Web server (although a number of operating system tricks can be used to mitigate the problem). Readers interested in the operating system issues surrounding persistent and non-persistent HTTP are encouraged to see [Nielsen 1997; Nahum 2002].

---

## Connectionless Transport: UDP

UDP, defined in [RFC 768], does just about as little as a transport protocol can do. Aside from the multiplexing/demultiplexing function and some light error checking, it adds nothing to IP. In fact, if the application developer chooses UDP instead of TCP, then the application is almost directly talking with IP. UDP takes messages from the application process, attaches source and destination port number fields for the multiplexing/demultiplexing service, adds two other small fields, and passes the resulting segment to the network layer. The network layer encapsulates the transport-layer segment into an IP datagram and then makes a best-effort attempt to deliver the segment to the receiving host. If the segment arrives at the receiving host, UDP uses the destination port number to deliver the segment’s data to the correct application process. Note that with UDP there is no handshaking between sending and receiving transport-layer entities before sending a segment. For this reason, UDP is said to be connectionless.

DNS is an example of an application-layer protocol that typically uses UDP. When the DNS application in a host wants to make a query, it constructs a DNS query message and passes the message to UDP. Without performing any handshaking with the UDP entity running on the destination end system, the host-side UDP adds header fields to the message and passes the resulting segment to the network layer. The network layer encapsulates the UDP segment into a datagram and sends the datagram to a name server. The DNS application at the querying host then waits for a reply to its query. If it doesn’t receive a reply (possibly because the underlying network lost the query or the reply), it might try resending the query, try sending the query to another name server, or inform the invoking application that it can’t get a reply.

Isn’t TCP always preferable, since TCP provides a reliable data transfer service, while UDP does not? The answer is no, as some applications are better suited for UDP for the following reasons:

- **Finer application-level control over what data is sent, and when.** Under UDP, as soon as an application process passes data to UDP, UDP will package the data inside a UDP segment and immediately pass the segment to the network layer.

- **No connection establishment.** TCP uses a three-way handshake before it starts to transfer data. UDP just blasts away without any formal preliminaries. This is probably the principal reason why DNS runs over UDP rather than TCP

- **No connection state.** TCP maintains connection state in the end systems. This connection state includes receive and send buffers, congestion-control parameters, and sequence and acknowledgment number parameters.

- **Small packet header overhead.** The TCP segment has 20 bytes of header overhead in every segment, whereas UDP has only 8 bytes of overhead.

| Application                   | Application-Layer Protocol | Underlying Transport Protocol   |
| ----------------------------- | -------------------------- | ------------------------------- |
| Electronic mail               | SMTP                       | TCP                             |
| Remote terminal access        | Telnet                     | TCP                             |
| Secure remote terminal access | SSH                        | TCP                             |
| Web                           | HTTP, HTTP/3               | TCP(for HTTP), UDP (for HTTP/3) |
| File transfer                 | FTP                        | TCP                             |
| Remote file server            | NFS                        | Typically UDP                   |
| Streaming multimedia          | DASH                       | TCP                             |
| Internet telephony            | typically proprietarty     | UDP or TCP                      |
| Network management            | SNMP                       | Typically UDP                   |
| Name translation              | DNS                        | Typically UDP                   |

Although commonly done today, running multimedia applications over UDP needs to be done with care. As we mentioned above, UDP has no congestion control. But congestion control is needed to prevent the network from entering a congested state in which very little useful work is done. If everyone were to start streaming high-bit-rate video without using any congestion control, there would be so much packet overflow at routers that very few UDP packets would successfully traverse the source-to-destination path. Moreover, the high loss rates induced by the uncontrolled UDP senders would cause the TCP senders to dramatically decrease their rates. Thus, the lack of congestion control in UDP can result in high loss rates between a UDP sender and receiver, and the crowding out of TCP sessions. Many researchers have proposed new mechanisms to force all sources, including UDP sources, to perform adaptive congestion control [Mahdavi 1997; Floyd 2000; Kohler 2006: RFC 4340].

#### UDP Segment Structure

The application data occupies the data field of the UDP segment. For example, for DNS, the data field contains either a query message or a response message. For a streaming audio application, audio samples fill the data field. The UDP header has only four fields, each consisting of two bytes. The checksum is used by the receiving host to check whether errors have been introduced into the segment. In truth, the checksum is also calculated over a few of the fields in the IP header in addition to the UDP segment. But we ignore this detail in order to see the forest through the trees.

#### UDP Checksum

The UDP checksum provides for error detection. That is, the checksum is used to determine whether bits within the UDP segment have been altered as it moved from source to destination.

![3.7.png](A:\PLAYGROUND\gettingstarted-computernetworking\a_top_down_approach\Images\3.7.png)

You may wonder why UDP provides a checksum in the first place, as many link-layer protocols (including the popular Ethernet protocol) also provide error checking. The reason is that there is no guarantee that all the links between source and destination provide error checking; that is, one of the links may use a link-layer protocol that does not provide error checking. Furthermore, even if segments are correctly transferred across a link, it’s possible that bit errors could be introduced when a segment is stored in a router’s memory. Given that neither link-by-link reliability nor in-memory error detection is guaranteed, UDP must provide error detection at the transport layer, on an end-end basis, if the end-end data transfer service is to provide error detection.

---

## Principles of Reliable Data Transfer

It is the responsibility of a **reliable data transfer protocol** to implement this service abstraction. This task is made difficult by the fact that the layer below the reliable data transfer protocol may be unreliable. example: TCP is a reliable data transfer protocol that is implemented on top of an unreliable (IP) end-to-end network layer. More generally, the layer beneath the two reliably communicating end points might consist of a single physical link (as in the case of a link-level data transfer protocol) or a global internetwork (as in the case of a transport-level protocol).

![3.8.png](A:\PLAYGROUND\gettingstarted-computernetworking\a_top_down_approach\Images\3.8.png)

#### Building a Reliable Data Transfer Protocol

###### Reliable Data Transfer over a Lossy Channel with Bit Errors: rdt1.0

![3.9.png](A:\PLAYGROUND\gettingstarted-computernetworking\a_top_down_approach\Images\3.9.png)

The sending side of rdt simply accepts data from the upper layer via the rdt_send(data) event, creates a packet containing the data (via the action make_pkt(data)) and sends the packet into the channel. In practice, the rdt_send(data) event would result from a procedure call (for example, to rdt_send()) by the upper-layer application.

On the receiving side, rdt receives a packet from the underlying channel via the rdt_rcv(packet) event, removes the data from the packet (via the action extract (packet, data)) and passes the data up to the upper layer (via the action deliver_data(data)). In practice, the rdt_rcv(packet) event would result from a procedure call (for example, to rdt_rcv()) from the lowerlayer protocol.

###### Reliable Data Transfer over a Lossy Channel with Bit Errors: rdt2.0

A more realistic model of the underlying channel is one in which bits in a packet may be corrupted. Such bit errors typically occur in the physical components of a network as a packet is transmitted, propagates, or is buffered.

Consider how you yourself might dictate a long message over the phone. In a typical scenario, the message taker might say “OK” after each sentence has been heard, understood, and recorded. If the message taker hears a garbled sentence, you’re asked to repeat the garbled sentence. This message-dictation protocol uses both **positive acknowledgments (“OK”)** and **negative acknowledgments**("Please repeat that."). These control messages allow the receiver to let the sender know what has been received correctly, and what has been received in error and thus requires repeating. In a computer network setting, reliable data transfer protocols based on such retransmission are known as **ARQ (Automatic Repeat reQuest) protocols**. 3 protocol capabilities ARQ protocols:

1. **Error detection.** These techniques require that extra bits (beyond the bits of original data to be transferred) be sent from the sender to the receiver; these bits will be gathered into the packet checksum field of the rdt2.0 data packet.

2. **Receiver feedback.** Since the sender and receiver are typically executing on different end systems, possibly separated by thousands of miles, the only way for the sender to learn of the receiver’s view of the world is for the receiver to provide explicit feedback to the sender.

3. **Retransmission.** A packet that is received in error at the receiver will be retransmitted by the sender.

![3.10.png](A:\PLAYGROUND\gettingstarted-computernetworking\a_top_down_approach\Images\3.10.png)

Protocol rdt2.1 uses both positive and negative acknowledgments from the receiver to the sender. When an out-of-order packet is received, the receiver sends a positive acknowledgment for the packet it has received. When a corrupted packet is received, the receiver sends a negative acknowledgment.

**Sender**

![3.11.png](A:\PLAYGROUND\gettingstarted-computernetworking\a_top_down_approach\Images\3.11.png)

**Receiver**

![3.12.png](A:\PLAYGROUND\gettingstarted-computernetworking\a_top_down_approach\Images\3.12.png)

###### Reliable Data Transfer over a Lossy Channel with Bit Errors: rdt3.0

Suppose now that in addition to corrupting bits, the underlying channel can lose packets as well, a not-uncommon event in today’s computer networks (including the Internet). Two additional concerns must now be addressed by the protocol: how to detect packet loss and what to do when packet loss occurs. The use of checksumming, sequence numbers, ACK packets, and retransmissions the techniques already developed in rdt2.2 will allow us to answer the latter concern. Handling the first concern will require adding a new protocol mechanism.

**rdt 2.2 sender**

![3.13.png](A:\PLAYGROUND\gettingstarted-computernetworking\a_top_down_approach\Images\3.13.png)

**rdt 2.2 Receiver**

![3.14.png](A:\PLAYGROUND\gettingstarted-computernetworking\a_top_down_approach\Images\3.14.png)

the sender FSM for rdt3.0, a protocol that reliably transfers data over a channel that can corrupt or lose packets; in the homework problems, you’ll be asked to provide the receiver FSM for rdt3.0.

![3.15.png](A:\PLAYGROUND\gettingstarted-computernetworking\a_top_down_approach\Images\3.15.png)

#### Pipelined Reliable Data Transfer Protocols

Protocol rdt3.0 is a functionally correct protocol, but it is unlikely that anyone would be happy with its performance, particularly in today’s high-speed networks. At the heart of rdt3.0’s performance problem is the fact that it is a stop-and-wait protocol.

![3.16.png](A:\PLAYGROUND\gettingstarted-computernetworking\a_top_down_approach\Images\3.16.png)

![3.17.png](A:\PLAYGROUND\gettingstarted-computernetworking\a_top_down_approach\Images\3.17.png)

![3.18.png](A:\PLAYGROUND\gettingstarted-computernetworking\a_top_down_approach\Images\3.18.png)

The solution to this particular performance problem is simple: Rather than operate in a stop-and-wait manner, the sender is allowed to send multiple packets without waiting for acknowledgments shows that if the sender is allowed to transmit three packets before having to wait for acknowledgments, the utilization of the sender is essentially tripled. Since the many in-transit sender-to-receiver packets can be visualized as filling a pipeline, this technique is known as pipelining. Pipelining has the following consequences for reliable data transfer protocols:

- The range of sequence numbers must be increased, since each in-transit packet
  (not counting retransmissions) must have a unique sequence number and there may be multiple, in-transit, unacknowledged packets.

- The sender and receiver sides of the protocols may have to buffer more than one packet. Minimally, the sender will have to buffer packets that have been transmitted but not yet acknowledged.

- The range of sequence numbers needed and the buffering requirements will depend on the manner in which a data transfer protocol responds to lost, corrupted, and overly delayed packets. Two basic approaches toward pipelined error recovery can be identified: **Go-Back-N** and **selective repeat**.

#### Go-Back-N(GBN)

In a Go-Back-N (GBN) protocol, the sender is allowed to transmit multiple packets (when available) without waiting for an acknowledgment, but is constrained to have no more than some maximum allowable number, N, of unacknowledged packets in the pipeline.

![3.19.png](A:\PLAYGROUND\gettingstarted-computernetworking\a_top_down_approach\Images\3.19.png)

The range of permissible sequence numbers for transmitted but not yet acknowledged packets can be viewed as a window of size N over the range of sequence numbers. As the protocol operates, this window slides forward over the sequence number space.

**sender**

![3.20.png](A:\PLAYGROUND\gettingstarted-computernetworking\a_top_down_approach\Images\3.20.png)

**receiver**

![3.21.png](A:\PLAYGROUND\gettingstarted-computernetworking\a_top_down_approach\Images\3.21.png)

give an extended FSM description of the sender and receiver sides of an ACK-based, NAK-free, GBN protocol. We refer to this FSM description as an extended FSM because we have added variables (similar to programming-language variables) for base and nextseqnum, and added operations on these variables and conditional actions involving these variables.

The implementation would also likely be in the form of various procedures that implement the actions to be taken in response to the various events that can occur. In such event-based programming, the various procedures are called (invoked) either by other procedures in the protocol stack, or as the result of an interrupt. In the sender, these events would be (1) a call from the upper-layer entity to invoke rdt_send(), (2) a timer interrupt, and (3) a call from the lower layer to invoke rdt_rcv() when a packet arrives.

![3.22.png](A:\PLAYGROUND\gettingstarted-computernetworking\a_top_down_approach\Images\3.22.png)

#### Selective Repeat(SR)

As the name suggests, selective-repeat protocols avoid unnecessary retransmissions by having the sender retransmit only those packets that it suspects were received in error (that is, were lost or corrupted) at the receiver. This individual, as-needed, retransmission will require that the receiver individually acknowledge correctly received packets. A window size of N will again be used to limit the number of outstanding, unacknowledged packets in the pipeline. However, unlike GBN, the sender will have already received ACKs for some of the packets in the window.

![3.23.png](A:\PLAYGROUND\gettingstarted-computernetworking\a_top_down_approach\Images\3.23.png)

![3.26.png](A:\PLAYGROUND\gettingstarted-computernetworking\a_top_down_approach\Images\3.26.png)

![3.27.png](A:\PLAYGROUND\gettingstarted-computernetworking\a_top_down_approach\Images\3.27.png)

**Summary of reliable data transfer**

| Mechanism               | Use, Comments                                                                                                                                                                                                                                                                                                                                                                                                                                |
| ----------------------- | -------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| Checksum                | Used to detect bit errors in a transmitted packet.                                                                                                                                                                                                                                                                                                                                                                                           |
| Timer                   | Used to timeout/retransmit a packet, possibly because the packet (or its ACK) was lost within the channel. Because timeouts can occur when a packet is delayed but not lost (premature timeout), or when a packet has been received by the receiver but the receiver-to-sender ACK has been lost, duplicate copies of a packet may be received by a receiver.                                                                                |
| Sequence number         | Used for sequential numbering of packets of data flowing from sender to receiver. Gaps in the sequence numbers of received packets allow the receiver to detect a lost packet. Packets with duplicate sequence numbers allow the receiver to detect duplicate copies of a packet                                                                                                                                                             |
| Acknowledgment          | Used by the receiver to tell the sender that a packet or set of packets has been received correctly. Acknowledgments will typically carry the sequence number of the packet or packets being acknowledged. Acknowledgments may be individual or cumulative, depending on the protocol.                                                                                                                                                       |
| Negative acknowledgment | Used by the receiver to tell the sender that a packet has not been received correctly. Negative acknowledgments will typically carry the sequence number of the packet that was not received correctly.                                                                                                                                                                                                                                      |
| Window, pipelining      | The sender may be restricted to sending only packets with sequence numbers that fall within a given range. By allowing multiple packets to be transmitted but not yet acknowledged, sender utilization can be increased over a stop-and-wait mode of operation. We’ll see shortly that the window size may be set on the basis of the receiver’s ability to receive and buffer messages, or the level of congestion in the network, or both. |

---

## Connection-Oriented Transport: TCP

#### The TCP Connection

TCP is said to be connection-oriented because before one application process can begin to send data to another, the two processes must first “handshake” with each 
other that is, they must send some preliminary segments to each other to establish the parameters of the ensuing data transfer. As part of TCP connection establishment, both sides of the connection will initialize many TCP state variables associated with the TCP connection.

> A TCP connection provides a **full-duplex service**: If there is a TCP connection between Process A on one host and Process B on another host, then application-layer data can flow from Process A to Process B at the same time as application-layer data flows from Process B to Process A. A TCP connection is also always **point-to-point**, that is, between a single sender and a single receiver. So-called “multicasting” the transfer of data from one sender to many receivers in a single send operation—is not possible with TCP. With TCP, two hosts are company and three are a crowd!

```python
clientSocket.connect((serverName,serverPort))
```

> TCP in the client then proceeds to establish a TCP connection with TCP in the server. For now it suffices to know that the client first sends a special TCP segment; the server responds with a second special TCP segment; and finally the client responds again with a third special segment. The first two segments carry no payload, that is, no application-layer data; the third of these segments may carry a payload. Because three segments are sent between the two hosts, this connection-establishment procedure is often referred to as a **three-way handshake**.

Once a TCP connection is established, the two application processes can send data to each other.

![3.28.png](A:\PLAYGROUND\gettingstarted-computernetworking\a_top_down_approach\Images\3.28.png)

TCP directs this data to the connection’s **send buffer**, which is one of the buffers that is set aside during the initial three-way handshake. TCP pairs each chunk of client data with a TCP header, thereby forming TCP segments. The segments are passed down to the network layer, where they are separately encapsulated within network-layer IP datagrams. The IP datagrams are then sent into the network.

#### TCP Segment Structure

The TCP segment consists of header fields and a data field. The data field contains a chunk of application data.

![3.29.png](A:\PLAYGROUND\gettingstarted-computernetworking\a_top_down_approach\Images\3.29.png)

A TCP segment header also contains the following fields:

- The 32-bit **sequence number field** and the 32-bit **acknowledgment number**
  field are used by the TCP sender and receiver in implementing a reliable data
  transfer service, as discussed below.

- The 16-bit **receive window** field is used for flow control.

- The 4-bit **header length field** specifies the length of the TCP header in 32-bit words. The TCP header can be of variable length due to the TCP options field (Typically, the options field is empty, so that the length of the typical TCP header is 20 bytes.)

- The optional and variable-length **options field** is used when a sender and receiver negotiate the maximum segment size (MSS) or as a window scaling factor for use in high-speed networks. A time-stamping option is also defined. See RFC 854 and RFC 1323 for additional details.

- The **flag field** contains 6 bits. The **ACK** bit is used to indicate that the value
  carried in the acknowledgment field is valid; that is, the segment contains an
  acknowledgment for a segment that has been successfully received. The **RST**, **SYN**, and **FIN** bits are used for connection setup and teardown, as we will discuss at the end of this section. The CWR and ECE bits are used in explicit congestion notification, as discussed in Section 3.7.2. Setting the **PSH** bit indicates that the receiver should pass the data to the upper layer immediately. Finally, the **URG** bit is used to indicate that there is data in this segment that the sending-side upperlayer entity has marked as “urgent.” The location of the last byte of this urgent data is indicated by the 16-bit **urgent data pointer field**. TCP must inform the receiving side upper-layer entity when urgent data exists and pass it a pointer to the end of the urgent data. (In practice, the PSH, URG, and the urgent data pointer are not used. However, we mention these fields for completeness.)

###### Sequence Numbers and Acknowledgment Numbers

Two of the most important fields in the TCP segment header are the sequence number field and the acknowledgment number field.

![3.30.png](A:\PLAYGROUND\gettingstarted-computernetworking\a_top_down_approach\Images\3.30.png)

TCP views data as an unstructured, but ordered, stream of bytes. TCP’s use of sequence numbers reflects this view in that sequence numbers are over the stream of transmitted bytes and not over the series of transmitted segments. The sequence **number for a segment** is therefore the byte-stream number of the first byte in the segment.

###### Telnet: A Case Study for Sequence and Acknowledgment Numbers

Telnet, defined in RFC 854, is a popular application-layer protocol used for remote login. It runs over TCP and is designed to work between any pair of hosts. Telnet example here, as it nicely illustrates TCP sequence and acknowledgment numbers. We note that many users now prefer to use the SSH protocol rather than Telnet, since data sent in a Telnet connection (including passwords!) are not encrypted, making Telnet vulnerable to eavesdropping attacks.

![3.31.png](A:\PLAYGROUND\gettingstarted-computernetworking\a_top_down_approach\Images\3.31.png)

#### Round-Trip Time Estimation and Timeout

###### Estimating the Round-Trip Time

###### Setting and Managing the Retransmission Timeout Interval

#### Reliable Data Transfer

TCP creates a reliable data transfer service on top of IP’s unreliable besteffort service. TCP’s reliable data transfer service ensures that the data stream that a process reads out of its TCP receive buffer is uncorrupted, without gaps, without duplication, and in sequence; that is, the byte stream is exactly the same byte stream that was sent by the end system on the other side of the connection.

![3.33.png](A:\PLAYGROUND\gettingstarted-computernetworking\a_top_down_approach\Images\3.33.png)

###### A Few Interesting Scenarios

![3.34.png](A:\PLAYGROUND\gettingstarted-computernetworking\a_top_down_approach\Images\3.34.png)

###### Doubling the Timeout Interval

The first concerns the length of the timeout interval after a timer expiration. In this modification, whenever the timeout event occurs, TCP retransmits the not-yet acknowledged segment with the smallest sequence number.

![3.35.png](A:\PLAYGROUND\gettingstarted-computernetworking\a_top_down_approach\Images\3.35.png)

![3.36.png](A:\PLAYGROUND\gettingstarted-computernetworking\a_top_down_approach\Images\3.36.png)

###### Fast Retransmit

One of the problems with timeout-triggered retransmissions is that the timeout period can be relatively long. When a segment is lost, this long timeout period forces the sender to delay resending the lost packet, thereby increasing the end-to-end delay. Fortunately, the sender can often detect packet loss well before the timeout event occurs by noting so-called duplicate ACKs.

**TCP ACK Generation Recommendation**

| Event                                                                                                                    | TCP Receiver Action                                                                                                                               |
| ------------------------------------------------------------------------------------------------------------------------ | ------------------------------------------------------------------------------------------------------------------------------------------------- |
| Arrival of in-order segment with expected sequence number. All data up to expected sequence number already acknowledged. | Delayed ACK. Wait up to 500 msec for arrival of another in-order segment. If next in-order segment does not arrive in this interval, send an ACK. |
| Arrival of in-order segment with expected sequence number. One other in-order segment waiting for ACK transmission.      | Immediately send single cumulative ACK, ACKing both in-order segments                                                                             |
| Arrival of out-of-order segment with higher-than-expected sequence number. Gap detected.                                 | Immediately send duplicate ACK, indicating sequence number of next expected byte (which is the lower end of the gap).                             |
| Arrival of segment that partially or completely fills in gap in received data.                                           | Immediately send ACK, provided that segment starts at the lower end of gap.                                                                       |

![3.37.png](A:\PLAYGROUND\gettingstarted-computernetworking\a_top_down_approach\Images\3.37.png)

###### Go-Back-N or Selective Repeat ?

A proposed modification to TCP, the so-called **selective acknowledgment** [RFC 2018], allows a TCP receiver to acknowledge out-of-order segments selectively rather than just cumulatively acknowledging the last correctly received, in-order segment. When combined with selective retransmission—skipping the retransmission of segments that have already been selectively acknowledged by the receiver TCP looks a lot like our generic SR protocol. Thus, TCP’s error-recovery mechanism is probably best categorized as a hybrid of GBN and SR protocols.

#### Flow Control

TCP provides a flow-control service to its applications to eliminate the possibility of the sender overflowing the receiver’s buffer. Flow control is thus a speedmatching service matching the rate at which the sender is sending against the rate at which the receiving application is reading. As noted earlier, a TCP sender can also be throttled due to congestion within the IP network; this form of sender control is referred to as congestion control.

#### TCP Connection Management

---

## Principles of Congestion Control

#### The Causes and the Costs of Congestion

In each case, we’ll look at why congestion occurs in the first place and at the cost of congestion (in terms of resources not fully utilized and poor performance received by the end systems). We’ll not (yet) focus on how to react to, or avoid, congestion but rather focus on the simpler issue of understanding what happens as hosts increase their transmission rate and the network becomes congested.

###### Scenario 1: Two Senders, a Router with Infinite Buffers

Two hosts (A and B) each have a connection that shares a single hop between source and destination.

###### Scenario 2: Two Senders and a Router with Finite Buffers

###### Scenario 3: Four Senders, Routers with Finite Buffers, and Multihop Paths

#### Approaches to Congestion Control

---

## TCP Congestion Control

#### The Causes and the Costs of Congestion

#### Approaches to Congestion Control

---

## Evolution of transport layer functionality

#### Classic TCP congestion Control

#### Network-Assisted Explicit Congestion Notification and Delay-based Congestion Control

#### Fairness

---

## Summary
